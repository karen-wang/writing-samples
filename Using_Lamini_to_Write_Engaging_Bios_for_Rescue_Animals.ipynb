{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karen-wang/writing-samples/blob/main/Using_Lamini_to_Write_Engaging_Bios_for_Rescue_Animals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lamini + Social Good:** Using Lamini to Write Engaging Bios for Rescue Animals\n",
        "_Karen Wang_ \\\n",
        "_Take-Home Test_ \\\n",
        "_July 10, 2023_"
      ],
      "metadata": {
        "id": "BzI1HC_k4DkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=12OMknjfUPaAQLPB3BBdX95sSZ1Jes6b8\" width=\"40%\" alt=\"Sleeping orange tabby cat.\">"
      ],
      "metadata": {
        "id": "MmU_VEDGQx0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lamini is a Python package that allows users to train and use large language models (LLMs) for their specific use cases. Lamini makes generative AI more accessible and affordable for everyone.\n",
        "\n",
        "Every year, millions of unwanted and abandoned animals are sent to shelters. In order to appeal to potential adopters, shelter workers must undergo the difficult and time-consuming task of writing engaging and unique bios for each animal. Today, we'll show you how to leverage the power of Lamini to make this task faster, easier, and more effective."
      ],
      "metadata": {
        "id": "Nxddw5jrMJKQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aimt2DaL36iT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @title Setup\n",
        "\n",
        "from google.colab import auth\n",
        "import requests\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "def authenticate_powerml():\n",
        "  auth.authenticate_user()\n",
        "  gcloud_token = !gcloud auth print-access-token\n",
        "  powerml_token_response = requests.get('https://api.powerml.co/v1/auth/verify_gcloud_token?token=' + gcloud_token[0])\n",
        "  return powerml_token_response.json()['token']\n",
        "\n",
        "production_token = authenticate_powerml()\n",
        "\n",
        "config = {\n",
        "    \"production\": {\n",
        "        \"key\": production_token,\n",
        "        \"url\": \"https://api.powerml.co\"\n",
        "    }\n",
        "}\n",
        "\n",
        "keys_dir_path = '/root/.powerml'\n",
        "os.makedirs(keys_dir_path, exist_ok=True)\n",
        "\n",
        "keys_file_path = keys_dir_path + '/configure_llama.yaml'\n",
        "with open(keys_file_path, 'w') as f:\n",
        "  yaml.dump(config, f, default_flow_style=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, authenticate with Lamini and install the necessary libraries."
      ],
      "metadata": {
        "id": "HgHXxfmsCbop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade llama-llm==0.0.15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "syRnBmF4Q1_o",
        "outputId": "a30c2d68-e979-48c4-e646-af237c7157a7",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-llm==0.0.15\n",
            "  Downloading llama_llm-0.0.15-4-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llama-llm==0.0.15) (1.10.9)\n",
            "Collecting python-configuration[yaml] (from llama-llm==0.0.15)\n",
            "  Downloading python_configuration-0.8.2-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from llama-llm==0.0.15) (2.27.1)\n",
            "Collecting tokenizers (from llama-llm==0.0.15)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from llama-llm==0.0.15) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-llm==0.0.15) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from llama-llm==0.0.15) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llama-llm==0.0.15) (4.6.3)\n",
            "Collecting pyyaml<6.0,>=5.1 (from python-configuration[yaml]->llama-llm==0.0.15)\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->llama-llm==0.0.15) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->llama-llm==0.0.15) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->llama-llm==0.0.15) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->llama-llm==0.0.15) (3.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->llama-llm==0.0.15) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->llama-llm==0.0.15) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->llama-llm==0.0.15) (3.1.0)\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45658 sha256=d5444abddd9803e1a51382445b5f845dda1375d8318ea0a091bbcd4fa4fdc757\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: tokenizers, pyyaml, python-configuration, llama-llm\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "Successfully installed llama-llm-0.0.15 python-configuration-0.8.2 pyyaml-5.4.1 tokenizers-0.13.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "v3uj4-2YtyoD",
        "outputId": "055ea9d1-424b-422d-bb1f-7179aa3a06ab",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "Successfully installed scipy-1.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, import and initialize the LLM. We want to give it a descriptive name; in our case we will use the LLM to generate bios for shelter animals, so we will name it `animal_bios`.\n",
        "\n"
      ],
      "metadata": {
        "id": "BLHufG7YInmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama import LLM\n",
        "\n",
        "llm = LLM(name=\"animal_bios\")"
      ],
      "metadata": {
        "id": "F67jTg5m4YBn"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, define the input and output type for your model.\n",
        "\n",
        "The LLM module is designed to be a general-purpose model that can work with any natural language input and output formats. One example would be a question and answer format, where you give a question to the LLM and it outputs an answer.\n",
        "\n",
        "For our purpose, we want the input to be information about an animal, and the output to be an engaging and descriptive bio of the animal.\n",
        "\n",
        "Lamini inputs and outputs are defined as Python classes that inherit from the library's `Type` class. Each class attribute provides `Context` for the model to know what you're trying to do with it.\n",
        "\n",
        "We define an input `Animal` and output `Bio` type below."
      ],
      "metadata": {
        "id": "blNUpmxMtF0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama import Type, Context\n",
        "\n",
        "# Input\n",
        "class Animal(Type):\n",
        "  species: str = Context(\"the species of the animal\")\n",
        "  breed: str = Context(\"the breed of the animal\")\n",
        "  sex: str = Context(\"the sex of the animal (male or female)\")\n",
        "  coat: str = Context(\"the animal's coat pattern and color\")\n",
        "\n",
        "# Output\n",
        "class Bio(Type):\n",
        "  name: str = Context(\"the name of the animal\")\n",
        "  description: str = Context(\"a description of the animal\")"
      ],
      "metadata": {
        "id": "soJ6wVmve7sa"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Animal` input includes information about the animal's species, breed, sex, and coat. From this, we want to generate an appropriate name and description for the animal's bio."
      ],
      "metadata": {
        "id": "o9NQSqH8FBHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animal = Animal(\n",
        "    species = \"cat\",\n",
        "    breed = \"mixed\",\n",
        "    sex = \"male\",\n",
        "    coat = \"orange tabby\"\n",
        ")\n",
        "\n",
        "animal_bio = llm(input=animal, output_type=Bio)\n",
        "print(f\"Name: {animal_bio.name}\")\n",
        "print(f\"Bio: {animal_bio.description}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7_j_ufYQSIwg",
        "outputId": "f3bcc013-6a6a-4639-8507-90c9145cab92"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Tiger\n",
            "Bio: Tiger is a handsome male orange tabby cat with a mixed breed. He has a soft, fluffy coat and bright, inquisitive eyes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try adding more complex types to the input. In addition to basic information about an animal, we can add information about the animal's personality, which can be important for potential adopters to know. We can define the personality type as a list of traits and add it to the `Animal` class."
      ],
      "metadata": {
        "id": "Xs6Oi_sLFUWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input\n",
        "class Personality(Type):\n",
        "  traits: list = Context(\"list of personality traits\")\n",
        "\n",
        "class Animal(Type):\n",
        "  species: str = Context(\"the species of the animal (dog or cat)\")\n",
        "  breed: str = Context(\"the breed of the animal\")\n",
        "  sex: str = Context(\"the sex of the animal (male or female)\")\n",
        "  coat: str = Context(\"the animal's coat pattern and color\")\n",
        "  personality: Personality = Context(\"the animal's personality\")"
      ],
      "metadata": {
        "id": "dfa_OvyWTc0_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's add some personality traits to our example and re-run the LLM."
      ],
      "metadata": {
        "id": "bvKVmldJGHZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "personality = Personality(\n",
        "    traits = [\"friendly\", \"likes chicken\"]\n",
        ")\n",
        "\n",
        "animal = Animal(\n",
        "    species = \"cat\",\n",
        "    breed = \"mixed\",\n",
        "    sex = \"male\",\n",
        "    coat = \"orange tabby\",\n",
        "    personality = personality\n",
        ")\n",
        "\n",
        "animal_bio = llm(input=animal, output_type=Bio)\n",
        "print(f\"Name: {animal_bio.name}\")\n",
        "print(f\"Bio: {animal_bio.description}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NHubW0uVT9oI",
        "outputId": "dfbf8fd7-65d9-4450-dda3-ecba8b39a8a1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Orangey\n",
            "Bio: Orangey is a friendly and curious male mixed cat with an orange tabby coat. He loves to explore and is particularly fond of chicken.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we've now specified that the animal is friendly and likes chicken, the LLM produces a bio that includes these traits. We can also tell the LLM to improve on certain criteria; for example, to add a note to potential adopters in the bio description."
      ],
      "metadata": {
        "id": "B4GztAviGO8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.improve(on=\"description\", to=\"add a note to potential adopters\")\n",
        "\n",
        "animal_bio = llm(input=animal, output_type=Bio)\n",
        "print(f\"Name: {animal_bio.name}\")\n",
        "print(f\"Bio: {animal_bio.description}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PDRj1fReZ1u5",
        "outputId": "e8a31c1e-36e3-443f-d893-6019ae97e33c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Orangey\n",
            "Bio: Orangey is a friendly and curious male mixed cat with an orange tabby coat. He loves to explore and is particularly fond of chicken. He is an affectionate and loyal companion and would make a great addition to any family.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since shelters deal with influxes of multiple animals at a time, we can use Lamini to generate multiple outputs at once. In our case, we can give it a list of cats and their corresponding information, and tell Lamini to do the work of generating bios for all of them."
      ],
      "metadata": {
        "id": "g7u7Vk9RP0Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cats = [\n",
        "    Animal(\n",
        "    species = \"cat\",\n",
        "    breed = \"mixed\",\n",
        "    sex = \"male\",\n",
        "    coat = \"orange tabby\",\n",
        "    personality = Personality(traits=[\"friendly\", \"likes chicken\"])\n",
        "  ),\n",
        "    Animal(\n",
        "    species = \"cat\",\n",
        "    breed = \"mixed\",\n",
        "    sex = \"male\",\n",
        "    coat = \"black\",\n",
        "    personality = Personality(traits=[\"easygoing\", \"likes fish\"])\n",
        "  ),\n",
        "    Animal(\n",
        "    species = \"cat\",\n",
        "    breed = \"mixed\",\n",
        "    sex = \"female\",\n",
        "    coat = \"orange tabby\",\n",
        "    personality = Personality(traits=[\"shy\", \"loves to cuddle\"])\n",
        "  )\n",
        "]\n",
        "\n",
        "for cat in cats:\n",
        "  animal_bio = llm(input=cat, output_type=Bio, random=True)\n",
        "  print(f\"Name: {animal_bio.name}\")\n",
        "  print(f\"Bio: {animal_bio.description}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VFapo71rV9Ig",
        "outputId": "34efae5c-a64b-492c-ec16-8ccbad162aff"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Orangey\n",
            "Bio: Orangey is a friendly and active male mixed cat with an orange tabby coat. He loves chicken and is always looking for a playmate. He is a great companion and would make a wonderful addition to any family.\n",
            "Name: Blackie\n",
            "Bio: Blackie is a male cat of a mixed breed with a black coat. He is an easygoing and fish-loving cat who would make a great companion for any family. Potential adopters should note that Blackie loves to cuddle and will often follow you around the house.\n",
            "Name: Candy\n",
            "Bio: Candy is a female orange tabby cat who is shy and loves to cuddle. She is a mixed breed with a beautiful coat and a sweet personality. She would make a great companion for someone looking for a loyal and loving pet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way of improving the LLM output is to give it existing data to learn from. In this case, we've taken data from the Humane Society of Silicon Valley (HSSV) website and given it to the LLM to train on. This will produce bios that are more aligned with the tone and style of HSSV."
      ],
      "metadata": {
        "id": "ZHTi0ULkQEIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: Humane Society of Silicon Valley\n",
        "animal = Animal(\n",
        "    species = \"cat\",\n",
        "    breed = \"mixed\",\n",
        "    sex = \"female\",\n",
        "    coat = \"siamese\",\n",
        "    personality = Personality(traits=[\"curious\",\n",
        "                                      \"high energy\",\n",
        "                                      \"likes snuggling\"])\n",
        "  )\n",
        "\n",
        "bio = Bio(\n",
        "    name = \"Bluebell\",\n",
        "    description = \"\"\"Hi there, Bluebell here. I'm a cute little kitty looking\n",
        "    for a forever home. I am curious and love exploring my surroundings.\n",
        "    Despite my high energy levels, I am still a lap kitty at heart and love\n",
        "    nothing more than snuggling up with my humans, esp if you massage my head\n",
        "    and neck. I am still learning my manners and have a tendency to give love\n",
        "    nibbles, but I am a quick learner and am eager to please. So come me visit\n",
        "    me soon.\"\"\"\n",
        ")\n",
        "\n",
        "data = [animal, bio]\n",
        "llm.add_data(data)"
      ],
      "metadata": {
        "id": "v2sZq79Dbigq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, we've used Lamini to develop a scalable solution for generating bios for shelter animals. We've also taught it to iteratively improve on certain criteria, by using both specific prompts and existing data.\n",
        "\n",
        "This is just one of the many ways that Lamini can be used for contributions to social good. Check out the [Lamini documentation](https://lamini-ai.github.io/) for more examples and ideas for future projects."
      ],
      "metadata": {
        "id": "THamAUbdQhwu"
      }
    }
  ]
}